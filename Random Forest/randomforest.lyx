#LyX 2.0 created this file. For more info see http://www.lyx.org/
\lyxformat 413
\begin_document
\begin_header
\textclass article
\use_default_options true
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman default
\font_sans default
\font_typewriter default
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100
\font_tt_scale 100

\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\use_hyperref false
\papersize default
\use_geometry false
\use_amsmath 1
\use_esint 1
\use_mhchem 1
\use_mathdots 1
\cite_engine basic
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\use_refstyle 1
\index Index
\shortcut idx
\color #008000
\end_index
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\quotes_language english
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Section*
Random Forest
\end_layout

\begin_layout Standard
En una serie de documentos e informes técnicos, Breiman (1996, 2000, 2001,
 2004) 
\begin_inset Note Note
status collapsed

\begin_layout Plain Layout
 poner cita.
 L.
 Breiman.
 Bagging predictors.
 Machine Learning, 24:123–140, 1996.
 L.
 Breiman.
 Some Infinity Theory for Predictor Ensembles.
 Technical Report 577, UC Berkeley, 2000.
 URL http://www.stat.berkeley.edu/ ˜ breiman.
 L.
 Breiman.
 Random forests.
 Machine Learning, 45:5–32, 2001.
 L.
 Breiman.
 Consistency For a Simple Model of Random Forests.
 Technical Report 670, UC Berkeley, 2004.
 URL http://www.stat.berkeley.edu/ ˜ breiman.
 L.
 Breiman, J.H.
 Friedman, R.A.
 Olshen, and C.J.
 Stone.
 Classification and Regression Trees.
 Chapman & Hall, New York, 1984.
\end_layout

\end_inset

 demostraron que hay ganancias sustanciales en la clasificación y la exactitud
 de la regresión y se puede lograr mediante el uso de conjuntos de árboles,
 donde cada árbol en el conjunto que crece de acuerdo con un parametro de
 azar.
 Las predicciones finales se obtienen de las agregaciones sobre el conjunto
 de datos.
 Como los componentes de base del conjunto son predictores con estructura
 de árbol, y desde cada uno de estos árboles se construye utilizando una
 inyección de aleatoriedad, estos procedimientos son llamados "Random Forest."
\end_layout

\begin_layout Standard

\end_layout

\begin_layout Standard
El random Forest es una combinación de árboles predictores tal que cada
 árbol depende de los valores de un vector aleatorio probado independientemente
 y con la misma distribución para cada uno de estos.
 Es una modificación sustancial de bagging que construye una larga colección
 de árboles no correlacionados y luego los promedia.
\end_layout

\begin_layout Standard
Random Forest es una unión entre métodos de clasificación y metodos de regresión
, esta opera por la construcción de una multitud de árboles de decisión
 en el entrenamiento y en la salida una clase el cual es el modo de salida
 de los árboles individuales.
\end_layout

\begin_layout Standard
El termino random forest proviene de los bosques de decisión aleatoria que
 fue propuesto por primera vez por estaño Jam Ho de laboratorios Bell en
 1995 y el algoritmo de random forest fue introducido y desarrollado por
 Leo Breiman y Adele Cutler.
\end_layout

\begin_layout Standard
Este método combina la idea de breiman del “bagging” y la selección aleatoria
 de características introducido de forma independiente por Ho concepto de
 (Amin y Geman) con el finde construir una colección de árboles de decisión
 con varianza controlada.
\end_layout

\begin_layout Standard
La selección de un subconjunto aleatorio de características es un ejemplo
 del método de subespacio aleatorio, que en la formulación de Ho, es una
 forma de implementar una clasificación propuesta por (Eugene Kleinberg).
 
\end_layout

\begin_layout Subsection*
Bagging o Bootstrap agregación
\end_layout

\begin_layout Standard
Este es un procedimiento aplicado en terminos generales, el procedimiento
 reduce la varianza de un método de aprendizaje estadístico, este es particular
 y frecuentemente usado en el contexto de árboles de decisión.
\end_layout

\begin_layout Standard
Es decir: Si tenemos un set de n observaciones Z1; : : : ;Zn, cada una con
 varianza 2, la varianza de la media de las observaciones Z estada do por
 2=n.
 en otras palabras, el promedio del set de observaciones reduce la varianza.
 por supuesto, esto no es practico porque generalmente tenemos acceso a
 múltiples sets de entrenamiento.
 En lugar, podemos iniciar bootstrap tomando muestras repetidas del set
 de datos de entrenamiento.
\end_layout

\begin_layout Standard
Esta características genera B diferentes datos de entrenamiento “bootstraped”
 nosotros entonces entrenaremos en el B_ceavo bootstraped set de entrenamiento
 en el orden a obtener 
\begin_inset Formula $\hat{f}^{*b}\left(x\right)$
\end_inset

, la predicción para el punto en X, tendríamos entonces un promedio de todas
 las predicciones posibles:
\end_layout

\begin_layout Standard
\begin_inset Formula $\hat{f}^{*b}\left(x\right)=\frac{1}{B}\overset{B}{\underset{b=1}{\sum}}\hat{f}^{*b}\left(x\right)$
\end_inset


\end_layout

\begin_layout Standard
A este procedimiento es a lo que llamamos bagging.
\end_layout

\begin_layout Standard
El Algoritmo de Random forest:
\end_layout

\begin_layout Standard
Proporcionan una mejora con respecto a los árboles en bagging a modo de
 pequeñas cantidades que descorrelacionan los árboles, esto reduce la varianza
 cuando promediamos los árboles.
 Al igual que en el bagging, construimos una serie de árboles de decisión
 sobre muestras de entrenamiento bootstrapped.
 Pero en la construcción de estos árboles de decisión, cada vez se da una
 
\begin_inset Quotes eld
\end_inset

separacion
\begin_inset Quotes erd
\end_inset

 en un árbol, esta es considerada, asi se realiza una una selección aleatoria
 de m predictores donde son elegido los candidatos de división de todo el
 conjunto de p predictores.
 Se le permite que la división use sólo uno de esos predictores m.
 en cada seleccion de las muestras boostraped se elige una selección fresca
 de m predictores que se toma para cada división, y por lo general elegimos
 
\begin_inset Formula $m\approx\sqrt{p}$
\end_inset

 es decir, el número de predictores considerados en cada división es aproximadam
ente igual a la raíz cuadrada del número total de predictores.
\end_layout

\end_body
\end_document
